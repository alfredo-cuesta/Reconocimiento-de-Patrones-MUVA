{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Autotutorial de preprocesado con _scikit-learn_\n",
    "\n",
    "En este cuaderno vamos a aprender a ver varios métodos de preprocesado que proporciona scikit-learn\n",
    "\n",
    "Puedes encontrar más información en la ayuda en-linea de _scikit-learn_\n",
    "\n",
    "---\n",
    "    [ES] Código de Alfredo Cuesta Infante para 'Reconocimiento de Patrones'\n",
    "       @ Master Universitario en Visión Artificial, 2020, URJC (España)\n",
    "    [EN] Code by Alfredo Cuesta-Infante for 'Pattern Recognition'\n",
    "       @ Master of Computer Vision, 2020, URJC (Spain)\n",
    "\n",
    "    alfredo.cuesta@urjc.es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234 #<- random generator seed (comment to get randomness)\n",
    "\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import sys\n",
    "sys.path.append('../../MyUtils/') #_ this is a package by myself with utils\n",
    "import MyUtils as my           #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comenzamos cargando datos y agrupándolos en 2 dataframes,  *X_full* e *Y_full*, para poder trabajar sobre ellos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Load data from CSV and put all in a single dataframe 'FullSet'\n",
    "\n",
    "FullSet_0 = pd.read_csv('../../Datasets/1000ceros.csv', header=None)\n",
    "FullSet_1 = pd.read_csv('../../Datasets/1000unos.csv',  header=None)\n",
    "FullSet = my.join_features_labels(FullSet_0,FullSet_1)\n",
    "\n",
    "#- Convert the 'FullSet' of pixels into the set 'X_full' of features and get the set 'Y_full' of labels\n",
    "\n",
    "theta = 0.5\n",
    "X_full = my.mnist_features( FullSet.drop('label', axis=1), theta=theta )\n",
    "Y_full = FullSet[['label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separación estratificada con selección aleatoria de elementos\n",
    "\n",
    "sklearn.model_selection.**StratifiedShuffleSplit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape is  (1600, 10)\n",
      "Y_train shape is  (1600, 1)\n",
      "X_test shape is  (400, 10)\n",
      "Y_test shape is  (400, 1)\n"
     ]
    }
   ],
   "source": [
    "#- Split X_full into TRAIN and TEST using scikit-learn\n",
    "\n",
    "test_size = 0.2 #- percentage of instances held for testing\n",
    "splitter = StratifiedShuffleSplit(n_splits=1,test_size=test_size, random_state=seed)\n",
    "#-> splitter is an object that will create ONE split, i.e. 2 subsets\n",
    "split_ix = splitter.split(X_full,Y_full)\n",
    "#-> method split needs the set of instances and the associated set of labels\n",
    "#-> it returns the indexes of the instances that go to the first subset and to the second subset\n",
    "for train_ix, test_ix in split_ix:\n",
    "    X_train = X_full.loc[train_ix].reset_index(drop=True) # remember that out inputs are dataframes\n",
    "    Y_train = Y_full.loc[train_ix].reset_index(drop=True) #  not numpy arrays !!\n",
    "    X_test  = X_full.loc[test_ix].reset_index(drop=True)  # -> that's why we have to write\n",
    "    Y_test  = Y_full.loc[test_ix].reset_index(drop=True)  #  so much !!\n",
    "\n",
    "print('X_train shape is ',X_train.shape)\n",
    "print('Y_train shape is ',Y_train.shape)\n",
    "print('X_test shape is ',X_test.shape)\n",
    "print('Y_test shape is ',Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count  0\n",
      "X_train shape is  (1600, 10)\n",
      "Y_train shape is  (1600, 1)\n",
      "X_test shape is  (400, 10)\n",
      "Y_test shape is  (400, 1)\n",
      "count  1\n",
      "X_train shape is  (1600, 10)\n",
      "Y_train shape is  (1600, 1)\n",
      "X_test shape is  (400, 10)\n",
      "Y_test shape is  (400, 1)\n"
     ]
    }
   ],
   "source": [
    "#- Split MANY TIMES X_full into TRAIN and TEST using scikit-learn\n",
    "\n",
    "test_size = 0.2 #- percentage of instances held for testing\n",
    "n_splits  = 2   #- NEW variable !! = number of times we make splitting\n",
    "splitter = StratifiedShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=seed)\n",
    "#-> splitter is an object that will create 2 subsets, 'n_splits' times.\n",
    "split_ix = splitter.split(X_full,Y_full)\n",
    "#-> method split needs the set of instances and the associated set of labels\n",
    "#-> it returns the indexes of the instances that go to the first subset and to the second subset\n",
    "\n",
    "count = 0 \n",
    "for train_ix, test_ix in split_ix:\n",
    "    print('count ',count); count+=1\n",
    "    \n",
    "    X_train = X_full.loc[train_ix].reset_index(drop=True)\n",
    "    Y_train = Y_full.loc[train_ix].reset_index(drop=True)\n",
    "    X_test  = X_full.loc[test_ix].reset_index(drop=True)\n",
    "    Y_test  = Y_full.loc[test_ix].reset_index(drop=True)\n",
    "    \n",
    "    print('X_train shape is ',X_train.shape)\n",
    "    print('Y_train shape is ',Y_train.shape)\n",
    "    print('X_test shape is ',X_test.shape)\n",
    "    print('Y_test shape is ',Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el resultado de la celda anterior podemos ver como la tarea de separar en dos subconjuntos (tanto los datos como las etiquetas) se ha realiado tantas veces como marca la variable **n_splits**.\n",
    "\n",
    "Por otra parte, como con cualquier objeto de Python, podemos ver los atributos de **spliter** invocando `.__dict__`, e incluso acceder a ellos, aunque normalmente también hay métodos (_getters_ y _setters_) para eso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attributes of splitter:  {'n_splits': 2, 'test_size': 0.2, 'train_size': None, 'random_state': 1234, '_default_test_size': 0.1} \n",
      "\n",
      "2 = how many times we are making a split, by reading the attribute \n",
      "\n",
      "2 = how many times we are making a split, by using the \"get\" method \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Attributes of splitter: ',splitter.__dict__,'\\n')\n",
    "print(splitter.n_splits, '= how many times we are making a split, by reading the attribute \\n')\n",
    "print(splitter.get_n_splits(),'= how many times we are making a split, by using the \"get\" method \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comentarios**\n",
    "\n",
    "+ Si sólo queremos hacer 1 división, por ej. TRAIN + TEST, es más rápido y sencillo utilizar **train_test_split**.<br>\n",
    "La división es estratificada.\n",
    "+ Si queremos hacer una división tipo *K-fold*, entences tenemos que utilizar **StratifiedKFold**, donde el parámetro *n_splits* es el número de divisiones que realizamos sobre el conjunto original. \n",
    "+ Consulta en [la ayuda en linea](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection) para ver otros métodos como **Leave-One-Out**, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_full,  Y_full, test_size=test_size, random_state=seed)\n",
    "\n",
    "# StratifiedKFold \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=n_splits)\n",
    "skf_ix = skf.split(X_full,Y_full)\n",
    "for train_ix, test_ix in split_ix:    \n",
    "    X_train = X_full.loc[train_ix].reset_index(drop=True)\n",
    "    Y_train = Y_full.loc[train_ix].reset_index(drop=True)\n",
    "    X_test  = X_full.loc[test_ix].reset_index(drop=True)\n",
    "    Y_test  = Y_full.loc[test_ix].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformación de características\n",
    "*Scikit-learn* proporciona una gran cantidad de métodos para transformar los conjuntos de datos.<br>\n",
    "Puedes consultar todos en la [ayuda en línea de *preprocesado*](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing)\n",
    "\n",
    "Los puntos más importantes son:\n",
    "+ Aquellas transformaciones en las que es necesario calcular algún parámetro para poder realizarlas, se deben hacer sólo sobre el conjunto de entrenamiento. Después esos parámetros se aplican sobre los demas conjuntos de datos.\n",
    "    > Por ej. Si queremos escalar los datos al intervalo $[0,1]$, tenemos que averiguar cuál es el valor máximo y mínimo del conjunto de entrenamiento, NO del conjunto que contenga todos los datos\n",
    "    <br> (el conjunto que contiene todos los datos NO existe, sería aquel que contenga todo lo que la máquina va a recibir en el futuro)\n",
    "+ Hay 3 métodos esenciales: *fit*, *transform* y *fit_transform*\n",
    "    - **fit** calcula los parámetros de la transformación a partir de los datos\n",
    "    - **transform** realiza la transformación con los parámetros calculados por fit\n",
    "    - **fit_transform** primero hace *fit* y luego *transform*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_unit = X_train_unit2\n",
      "X_test_unit = X_test_unit2\n"
     ]
    }
   ],
   "source": [
    "#- Select the features and scale to [0,1] with respect to the TRAIN set\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "feat_selec= ['width' , 'height']\n",
    "\n",
    "# Doing \"fit\" and \"transform\"\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train[feat_selec])  #<- fit = get the parameters of the scaling\n",
    "X_train_unit = scaler.transform(X_train[feat_selec]) #<- transform X_train \n",
    "X_test_unit  = scaler.transform(X_test[feat_selec])  #<- transform X_test\n",
    "\n",
    "# Doing \"fit_transform\" and \"transform\"\n",
    "scaler2 = MinMaxScaler()\n",
    "X_train_unit2 = scaler2.fit_transform(X_train[feat_selec]) #<-fit & transform X_train in 1 step\n",
    "X_test_unit2  = scaler2.fit_transform(X_test[feat_selec])  #<- transform X_test\n",
    "\n",
    "# checking that we get the same outcomes -\n",
    "if np.sum(X_train_unit - X_train_unit2) == 0 :\n",
    "    print('X_train_unit = X_train_unit2')\n",
    "if np.sum(X_test_unit - X_test_unit2) == 0 :\n",
    "    print('X_test_unit = X_test_unit2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cometarios**\n",
    "+ Los objetos de sklearn.preprocessing devuelven numpy arrays<br>\n",
    "  $\\rightarrow$ Si queremos seguir trabajando con dataframes de pandas habrá que construirlo a partir de la salida y de las características elegidas\n",
    "+ Recuerda que `.__dict__` devuelve los atributos del objeto "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.944444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.526316</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      width    height\n",
       "0  0.789474  0.833333\n",
       "1  0.526316  0.944444\n",
       "2  0.157895  0.888889\n",
       "3  0.684211  0.888889\n",
       "4  0.526316  1.000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from numpy array to pandas dataframe \n",
    "X_train_unit_df = pd.DataFrame(X_train_unit, columns = feat_selec)\n",
    "X_train_unit_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feature_range': (0, 1),\n",
       " 'copy': True,\n",
       " 'n_features_in_': 2,\n",
       " 'n_samples_seen_': 1600,\n",
       " 'scale_': array([0.05263158, 0.05555556]),\n",
       " 'min_': array([ 0.        , -0.05555556]),\n",
       " 'data_min_': array([0., 1.]),\n",
       " 'data_max_': array([19., 19.]),\n",
       " 'data_range_': array([19., 18.])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio\n",
    "1. Crear un conjunto de datos utilizando las características **W_max1** y **area** \n",
    "2. Realizar un 5-fold de tal manera que:\n",
    "    - En cada iteración se escalen las características respecto al conjunto de entrenamiento al intervalo $[0,1]$\n",
    "    - Se aprenda un clasificador lineal por regresión logística\n",
    "    - Se evalue clasificador con el conjunto de test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solución "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comentarios**\n",
    "+ Normalmente los resultados de un fold a otro varían \"ligeramente\", **¿con cuál me quedo?**\n",
    "+ Lo mejor es quedarse con aquel modelo que esté más próximo al promedio, ni con el \"mejor\" ni con el \"peor\".<br>\n",
    "  $\\rightarrow$ es conveniente guardar el modelo de cada _fold_ en una lista, para después recuperar el elegido.\n",
    "+ <u>Importante.</u> Al hacer validación cruzada no necesitamos separar el conjunto de datos inicial en *TRAIN-VALIDATION-TEST* porque el mismo procedimiento va obtiendo *TRAIN* y *VALIDATION* en cada _fold_.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
